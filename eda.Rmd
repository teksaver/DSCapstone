---
title: "EDA"
author: "Sylvain Tenier"
date: "10 juin 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Executive summary

We analyse 3 files containing text in English from blogs, news and Twitter. We first summarize the main characteristics of each file. We then use "vanilla" R tools to detect numbers, profanity and tokenize each line of the corpus. Using the tokenized version, we show that the population respects the power law distribution. Finally, we present our plan and tools for a word prediction appication from this corpus.

# Main characteristics

## Number of lines for each data set

We first check whether the dataset can fit in memory. Using the `readLines` function from a `connection` removes any *null characters* warnings and loads all the data without trouble.

```{r file load, cache=TRUE}
blogsConn <- file("./final/en_US/en_US.blogs.txt","r")
newsConn <- file("./final/en_US/en_US.news.txt","r")
twittConn <- file("./final/en_US/en_US.twitter.txt","r")
blogsLines <- readLines(blogsConn,n=-1,skipNul = TRUE)
newsLines <- readLines(newsConn,n=-1,skipNul = TRUE)
twittLines <- readLines(twittConn,n=-1,skipNul = TRUE)
close(blogsConn)
close(newsConn)
close(twittConn)
```

During the analysis, tests are performed using a subset of the data using the `sample` function such as
```{r eval=FALSE}
blogsLines[sample(1:length(blogsLines), length(blogsLines)/10, replace=FALSE)]
```
The results presented in this report are generated from the full data set. This is reasonnable since the computations can be carried out on a nine-years old laptop (intel i3, 4GB RAM).

```{r summary}
library(knitr)
summBlog <- as.character(summary(blogsLines))
summNews <- as.character(summary(newsLines))
summTwitt <- as.character(summary(twittLines))
summDf <- data.frame(source="Blog",length=summBlog[1],class=summBlog[2])
summDf <-rbind(summDf,data.frame(source="News",length=summNews[1],class=summNews[2]))
summDf <-rbind(summDf,data.frame(source="Twitter",length=summTwitt[1],class=summTwitt[2]))
kable(summDf)
```

## Distribution of number of characters per line

We plot a histogram for the distribution of number of characters per line for each kind of data. We can see that blog and news data follow a normal distribution, while Twitter's 160 chararacters-per-message limit causes a strong bias towards that limit.

```{r char distribution, cache=TRUE}
suppressPackageStartupMessages(library(ggplot2))
charCountDF <- data.frame(nbChar=integer(),source=factor(levels=c("Blog","News","Twitter")))
charCountDF <- rbind(charCountDF,data.frame(nbChar=sapply(blogsLines,nchar),source="Blog"))
charCountDF <- rbind(charCountDF,data.frame(nbChar=sapply(newsLines,nchar),source="News"))
charCountDF <- rbind(charCountDF,data.frame(nbChar=sapply(twittLines,nchar),source="Twitter"))
ggplot(data=charCountDF,aes(nbChar))+
    facet_grid(. ~ source)+
    geom_histogram()+
    scale_x_log10() +
    labs(x="Number of characters",title="Number of characters per line")
```

# Content analysis

We first need to construct a function to generate a regular expression for profanity detection.

```{r profanity regexp}
prof <- scan("naughty.txt", what="", sep="\n")
profExpr <- " ("
for (item in prof){
    profExpr <- paste0(profExpr,item, sep = "|", collapse = NULL)
}
profExpr <- paste0(profExpr,")(s|ing|ed)* ",sep="",collapse = NULL)
```

We now tokenize the content to analyse the distribution of words for each kind of data.

```{r tokenizer function, cache=TRUE}
tokenize <- function(line){
    #replace all punctuation with space
    line <- gsub("[[:punct:]]"," ",line)
    #merge multiple spaces
    line <- gsub("\\s+", " ",line)
    #generalize numbers with <number> token
    line <- gsub(" [[:digit:]]+ "," <number> ",line)
    #replace profanity with <profanity>
    line <- gsub(profExpr," <profanity> ",line)
    #split by space
    tokens <- strsplit(line, split="[[:space:]]")[[1]]
    return(tokens)
}
```

# Plans for the Shiny prediction app

## Dealing with limited resources

The data used for this analysis was small enough to fit in memory. If needed for computational reasons, specially given the limited resources at shiny.io, we will use the following strategies:

- Subsetting strategies :as seen in the inference class of the specialization, since the lines follow a normal distribution (as shown in the first part of this analysis) we can take multiple subsamples of the data to get a good approximation of the results on the full dataset
- Reduction strategy: since the word occurence distribution follows a power law, we can use a small subest of the most-occuring words and still have a high coverage. Least common words can be factorized as <rare> exactly like all numbers are factorized into <number> and profanities as <profanity>.


